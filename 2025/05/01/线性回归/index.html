<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>线性回归 | Sre.Night's blog</title><meta name="author" content="Sre.Night"><meta name="copyright" content="Sre.Night"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#F5F9FF"><meta name="description" content="线性回归线性回归介绍线性回归线性回归(Linear regression)是利用 回归方程(函数) 对 一个或多个自变量(特征值)和因变量(目标值)之间 关系进行建模的一种分析方式。  .vxnysdpterwn{zoom:150%;}    注意事项：  求解的w，都是w的零次幂，所以叫线性模型  在线性回归中，从数据中获取的规律其实就是学习权重系数w  某一个权重值w越大，说明这个权重的数据影">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归">
<meta property="og:url" content="https://sre-night.github.io/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.html">
<meta property="og:site_name" content="Sre.Night&#39;s blog">
<meta property="og:description" content="线性回归线性回归介绍线性回归线性回归(Linear regression)是利用 回归方程(函数) 对 一个或多个自变量(特征值)和因变量(目标值)之间 关系进行建模的一种分析方式。  .vxnysdpterwn{zoom:150%;}    注意事项：  求解的w，都是w的零次幂，所以叫线性模型  在线性回归中，从数据中获取的规律其实就是学习权重系数w  某一个权重值w越大，说明这个权重的数据影">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sre-night.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-05-01T11:25:32.000Z">
<meta property="article:modified_time" content="2025-07-03T10:17:19.798Z">
<meta property="article:author" content="Sre.Night">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sre-night.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "线性回归",
  "url": "https://sre-night.github.io/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/",
  "image": "https://sre-night.github.io/img/butterfly-icon.png",
  "datePublished": "2025-05-01T11:25:32.000Z",
  "dateModified": "2025-07-03T10:17:19.798Z",
  "author": [
    {
      "@type": "Person",
      "name": "Sre.Night",
      "url": "https://sre-night.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://sre-night.github.io/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0A1A35')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#F5F9FF')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '线性回归',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" href="/css/loading.css"><link rel="stylesheet" href="/css/nav.css"><link rel="stylesheet" href="/css/footer.css"><link rel="stylesheet" href="/css/fonts/font.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-categories-card@1.0.0/lib/categorybar.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/xlenco/JS-X@main/pace.js/pace.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/default_top_img.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Sre.Night's blog</span></a><a class="nav-page-title" href="/"><span class="site-name">线性回归</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">线性回归</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-01T11:25:32.000Z" title="发表于 2025-05-01 19:25:32">2025-05-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-03T10:17:19.798Z" title="更新于 2025-07-03 18:17:19">2025-07-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">4.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h1 id="线性回归介绍"><a href="#线性回归介绍" class="headerlink" title="线性回归介绍"></a>线性回归介绍</h1><h3 id="线性回归-1"><a href="#线性回归-1" class="headerlink" title="线性回归"></a>线性回归</h3><p>线性回归(Linear regression)是利用 <strong>回归方程(函数)</strong> 对 <strong>一个或多个自变量(特征值)和因变量(目标值)之间</strong> 关系进行建模的一种分析方式。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20230901102250602.png" alt="image-20230901102250602"></p>
<style>.vxnysdpterwn{zoom:150%;}</style><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20230901102402944.png" class="vxnysdpterwn" alt="image-20230901102402944">



<p>注意事项：</p>
<ul>
<li><p>求解的w，都是<strong>w的零次幂</strong>，所以叫线性模型</p>
</li>
<li><p>在线性回归中，从数据中获取的<strong>规律</strong>其实就是<strong>学习权重系数w</strong></p>
</li>
<li><p>某一个权重值<strong>w越大</strong>，说明这个权重的数据<strong>影响越大</strong></p>
</li>
</ul>
<h3 id="线性回归分类"><a href="#线性回归分类" class="headerlink" title="线性回归分类"></a>线性回归分类</h3><ul>
<li>一元线性回归：y &#x3D; kx +b</li>
</ul>
<p>目标值只与一个<strong>因变量</strong>有关系</p>
<ul>
<li>多元线性回归：</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20230901102940614.png" alt="image-20230901102940614"></p>
<p>目标值只与<strong>多个因变量</strong>有关系</p>
<h1 id="线归问题求解"><a href="#线归问题求解" class="headerlink" title="线归问题求解"></a>线归问题求解</h1><h3 id="线归API的应用"><a href="#线归API的应用" class="headerlink" title="线归API的应用"></a>线归API的应用</h3><p>预测身高</p>
<p>已知数据:</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20230901104147248.png" alt="image-20230901104147248"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20230901104237860.png" alt="image-20230901104237860"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 导入线性回归包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_height</span>():</span><br><span class="line">    <span class="comment"># 2. 准备数据（身高cm和体重kg）</span></span><br><span class="line">    x = [[<span class="number">160</span>], [<span class="number">165</span>], [<span class="number">172</span>], [<span class="number">174</span>],[<span class="number">180</span>]]  <span class="comment"># 二维特征数据</span></span><br><span class="line">    y = [<span class="number">56.3</span>, <span class="number">60.6</span>, <span class="number">65.1</span>, <span class="number">68.5</span>, <span class="number">75</span>]         <span class="comment"># 标签数据</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 实例化线性回归模型</span></span><br><span class="line">    estimator = LinearRegression()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 训练线性回归模型</span></span><br><span class="line">    estimator.fit(x, y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 打印模型参数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;estimator.coef_:&#x27;</span>, estimator.coef_)       <span class="comment"># 系数w</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;estimator.intercept_:&#x27;</span>, estimator.intercept_)  <span class="comment"># 截距b</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 5. 模型预测</span></span><br><span class="line">    myres = estimator.predict([[<span class="number">176</span>]])  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;myres-&gt;&#x27;</span>, myres)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行函数</span></span><br><span class="line">predict_height()</span><br></pre></td></tr></table></figure>



<h3 id="损失函数-代价函数"><a href="#损失函数-代价函数" class="headerlink" title="损失函数(代价函数)"></a>损失函数(代价函数)</h3><p>想求一条直线<strong>更好的拟合所有点</strong><br>$$<br>y &#x3D; kx + b<br>$$</p>
<ul>
<li><p>引入损失函数(衡量预测值和真实值效果) Loss(k, b) </p>
</li>
<li><p>通过一个优化方法，求损失函数最小值，得到K最优解</p>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/image-20230901110253313.png" alt="image-20230901110253313"></p>
<ol>
<li><p><strong>误差概念</strong>：用 <strong>[预测值y – 真实值y]</strong> 就是<strong>误差</strong></p>
</li>
<li><p><strong>损失函数</strong>：衡量每个样本预测值与真实值效果的函数</p>
</li>
<li><p>“<strong>红色直线</strong>能更好的拟合所有点”也就是<strong>误差最小</strong>，<strong>误差和最小</strong></p>
</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/%7BA6ABC670-3838-4E8E-AB69-28FDB106F7AE%7D.png" alt="{A6ABC670-3838-4E8E-AB69-28FDB106F7AE}"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/%7BA6ABC670-3838-4E8E-AB69-28FDB106F7AE%7D.png" alt="{A6ABC670-3838-4E8E-AB69-28FDB106F7AE}"></p>
<p><strong>损失函数</strong>是关于<strong>𝑘、𝑏</strong>的函数，展开会变成<strong>二元二次方程</strong>。 为简化计算，先固定截距𝑏 ,<strong>x&#x3D;0时</strong>，𝑏可设置成一个负值，<strong>𝑏 固定成-100</strong></p>
<p>当损失函数取最小值时，得到k就是最优解</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/%7BC6F1ED4F-01F8-4E67-95D6-75E5080FE7A0%7D.png" alt="{C6F1ED4F-01F8-4E67-95D6-75E5080FE7A0}"></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./assets/{217B33E7-CA3C-4F48-830D-1AAD43F8FF09}.png" alt="{217B33E7-CA3C-4F48-830D-1AAD43F8FF09}" style="zoom:80%;" />

<p>回归的损失函数：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./assets/{C37267B6-9F40-4AF6-B323-6750DB145606}.png" alt="{C37267B6-9F40-4AF6-B323-6750DB145606}" style="zoom:50%;" />

<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="./assets/{1C6A59EE-DB05-4790-A218-DF2E44ED3E9E}.png" alt="{1C6A59EE-DB05-4790-A218-DF2E44ED3E9E}" style="zoom:80%;" />

<h3 id="导数和矩阵"><a href="#导数和矩阵" class="headerlink" title="导数和矩阵"></a>导数和矩阵</h3><h4 id="常见的数据表述"><a href="#常见的数据表述" class="headerlink" title="常见的数据表述"></a>常见的数据表述</h4><ul>
<li><p><strong>标量scalar</strong> :一个独立存在的数，只有大小没有方向</p>
</li>
<li><p><strong>向量vector</strong> :向量指一列顺序排列的元素。默认是列向量</p>
</li>
<li><p><strong>矩阵matrix</strong> :二维数组</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901115232323.png" alt="image-20230901115232323"></p>
</li>
<li><p><strong>张量Tensor</strong> :多维数组，张量是基于<strong>向量和矩阵的推广</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901115246275.png" alt="image-20230901115246275"></p>
</li>
</ul>
<h4 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h4><p>高数上定义</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/%E5%AF%BC%E6%95%B0.jpeg"></p>
<p>导数是函数的局部性质。一个函数在某一点的导数描述了这个函数在这一点附近的变化率。</p>
<p>函数在某一点的导数就是该函数所代表的曲线在这一点上的切线斜率</p>
<p><strong>需掌握的</strong></p>
<ul>
<li>常见求导公式</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20240731191353155-17224246394383.png" alt="image-20240731191353155"></p>
<ul>
<li><p>四则运算</p>
</li>
<li><p>复合函数求导</p>
</li>
<li><p>导数求极值：导数为0的位置是函数的极值点</p>
</li>
</ul>
<h4 id="偏导"><a href="#偏导" class="headerlink" title="偏导"></a>偏导</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901144053678.png" alt="image-20230901144053678"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901144102962.png" alt="image-20230901144102962"></p>
<h4 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h4><ul>
<li>有大小方向</li>
<li>线性运算</li>
<li>转置(Transpose)</li>
<li>范数(norm)：基本概念，具有长度的意义<ul>
<li>1范数(L1范数)：向量中各个元素<strong>绝对值之和</strong></li>
<li>2范数(L2范数)：向量的模长，各个元素<strong>平方求和开根号</strong></li>
<li>p范数(Lp范数)：向量各个元素<strong>p次幂求和，开p次根</strong></li>
</ul>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/%7B35ADFD6C-D1C7-4863-A57C-B1E4F9CF615E%7D.png" alt="{35ADFD6C-D1C7-4863-A57C-B1E4F9CF615E}"></p>
<h4 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h4><ul>
<li><p>机器学习中的表达：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/%7B20EDA20B-2FBC-4489-AFFC-35FE57BFB561%7D.png" alt="{20EDA20B-2FBC-4489-AFFC-35FE57BFB561}"></p>
</li>
<li><p>线性、乘法运算，乘法性质</p>
</li>
<li><p>转置极其性质</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/%7B2B607A6E-3282-49A7-B989-D36CFF2B39CE%7D.png" alt="{2B607A6E-3282-49A7-B989-D36CFF2B39CE}"></p>
</li>
<li><p>矩阵@矩阵的转置</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/%7BE0CB5276-6AF5-4A5C-A88A-78899D52F00F%7D.png" alt="{E0CB5276-6AF5-4A5C-A88A-78899D52F00F}"></p>
</li>
<li><p>方针、单位阵</p>
</li>
<li><p>逆矩阵</p>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901145840373.png" alt="image-20230901145840373"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901150113602.png" alt="image-20230901150113602"></p>
<h3 id="一元线性回归的解析解"><a href="#一元线性回归的解析解" class="headerlink" title="一元线性回归的解析解"></a>一元线性回归的解析解</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901150406897.png" alt="image-20230901150406897"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/image-20250421195422133.png" alt="image-20250421195422133"></p>
<p><strong>步骤</strong></p>
<ul>
<li>用<strong>最小二乘法</strong>列出<strong>二元函数</strong></li>
<li>分别对<strong>k和b求偏导</strong></li>
<li>代入数据计算<strong>解出k和b</strong></li>
<li>得到<strong>预测值</strong></li>
</ul>
<h3 id="多元线性回归-正规方程法"><a href="#多元线性回归-正规方程法" class="headerlink" title="多元线性回归-正规方程法"></a>多元线性回归-正规方程法</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/image-20250421203032773.png" alt="image-20250421203032773"></p>
<ul>
<li>损失函数普通方式转成矩阵方式</li>
</ul>
<p>$$<br>J(w) &#x3D; \sum_{i&#x3D;1}^m \left( h(x_i) - y_i \right)^2 &#x3D; | Xw - y |_2^2<br>$$</p>
<p><strong>Step 1：展开形式</strong><br>$$<br>J(w) &#x3D; \underbrace{(h(x_1) - y_1)^2}<em>{\text{样本1的误差}} + \underbrace{(h(x_2) - y_2)^2}</em>{\text{样本2的误差}} + \cdots + \underbrace{(h(x_m) - y_m)^2}_{\text{样本m的误差}}<br>$$</p>
<p><strong>Step 2：求和符号简写</strong><br>$$<br>J(w) &#x3D; \sum_{i&#x3D;1}^m \left( h(x_i) - y_i \right)^2<br>$$</p>
<p><strong>Step 3：矩阵形式转换</strong><br>$$<br>J(w) &#x3D; | Xw - y |_2^2<br>$$</p>
<p>• 推导过程：</p>
<ol>
<li><p>设计矩阵构造：<br>将所有样本特征  xi 堆叠成设计矩阵<br>$$<br>X \in \mathbb{R}^{m \times n}<br>$$<br>其中：<br>$$<br>X &#x3D; \begin{bmatrix}<br>x_1^T \<br>x_2^T \<br>\vdots \<br>x_m^T<br>\end{bmatrix}, \quad<br>y &#x3D; \begin{bmatrix}<br>y_1 \<br>y_2 \<br>\vdots \<br>y_m<br>\end{bmatrix}<br>$$</p>
</li>
<li><p>预测向量表示：<br>$$<br>Xw &#x3D; \begin{bmatrix}<br>   x_1^T w \<br>   x_2^T w \<br>   \vdots \<br>   x_m^T w<br>   \end{bmatrix} &#x3D; \begin{bmatrix}<br>   h(x_1) \<br>   h(x_2) \<br>   \vdots \<br>   h(x_m)<br>   \end{bmatrix}<br>$$</p>
</li>
<li><p>向量差与范数：  </p>
<p>  $$<br>  | Xw - y |<em>2^2 &#x3D; (Xw - y)^T (Xw - y) &#x3D; \sum</em>{i&#x3D;1}^m (h(x_i) - y_i)^2<br>  $$</p>
</li>
</ol>
<p>$$<br>J(w) &#x3D; (h(x_1)−y_1)^2 + (h(x_2)−y_2)^2 + \cdots + (h(x_m)−y_m)^2 \<br>&#x3D; \sum_{i&#x3D;1}^m (h(x_i) − y_i)^2 &#x3D; | Xw − y |_2^2<br>$$</p>
<hr>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230911234116911.png" alt="image-20230911234116911"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901152745308.png" alt="image-20230901152745308"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901152758396.png" alt="image-20230901152758396"></p>
<h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><h4 id="梯度下降思想"><a href="#梯度下降思想" class="headerlink" title="梯度下降思想"></a>梯度下降思想</h4><ul>
<li>求解函数极值还有更通用的方法就是<strong>梯度下降法</strong>。顾名思义：<strong>沿着梯度下降的方向</strong>求解极小值</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901183007785.png" alt="image-20230901183007785"></p>
<ul>
<li>输入：初始化位置S;每步距离为a 。输出:从位置S到达山底</li>
<li>步骤1：令初始化位置为山的任意位置S</li>
<li>步骤2：在当前位置环顾四周，如果四周都比S高返回S;否则执行步骤3</li>
<li>步骤3：在当前位置环顾四周，寻找坡度最陡的方向，令其为x方向</li>
<li>步骤4：沿着x方向往下走，长度为a，到达新的位置S‘ </li>
<li>步骤5：在S‘位置环顾四周，如果四周都比 S‘ 高，则返回S‘ 。否则转到步骤3</li>
</ul>
<p>小结:通过<strong>循环迭代</strong>的方法<strong>不断更新位置S</strong> (相当于<strong>不断更新权重参数w</strong>)</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901183020726.png" alt="image-20230901183020726"></p>
<p> 最终找到最优解，这个方法可用来求损失函数最优解， 比<strong>正规方程更通用</strong></p>
<blockquote>
<p>梯度下降过程就和下山场景类似可微分的损失函数，代表着一座山<br>寻找的函数的最小值，也就是山底</p>
</blockquote>
<ul>
<li><p>什么是梯度</p>
<ul>
<li>单变量函数中，梯度就是<strong>某一点切线斜率</strong>（某一点的导数）；梯度方向为<strong>函数增长最快的方向</strong></li>
<li>多变量函数中，梯度就是<strong>某一个点的偏导数</strong>；有方向：偏导数分量的<strong>向量方向</strong></li>
</ul>
</li>
<li><p>梯度下降公式</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/%7B85FD3246-F8A8-4397-A7CE-E5083E723E4F%7D.png" alt="{85FD3246-F8A8-4397-A7CE-E5083E723E4F}"></p>
<ul>
<li>α：<strong>学习率(步长)</strong>， 不能太大, 也不能太小. 机器学习中：<strong>0.001 ~ 0.01</strong> </li>
<li>梯度是<strong>上升最快</strong>的方向, 我们需要是下降最快的方向, 所以<strong>需要加负号</strong></li>
</ul>
</li>
<li><p>举个栗子</p>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/%7BBDE650B1-A2EB-4E9A-A2FB-83919B2F683B%7D.png" alt="{BDE650B1-A2EB-4E9A-A2FB-83919B2F683B}"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/%7B854E4731-DD44-4C5B-A14E-E7DBD717C574%7D.png" alt="{854E4731-DD44-4C5B-A14E-E7DBD717C574}"></p>
<ul>
<li>步长<ul>
<li>步长决定了在梯度下降迭代的过程中，每一步沿梯度<strong>负方向</strong>前进的长度 </li>
<li>2.学习率太小，下降的<strong>速度会慢</strong>  </li>
<li>3.学习率太大：容易造成<strong>错过最低点</strong>、产生下降过程中的震荡、甚至<strong>梯度爆炸</strong></li>
</ul>
</li>
</ul>
<h4 id="银行信贷案例"><a href="#银行信贷案例" class="headerlink" title="银行信贷案例"></a>银行信贷案例</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901183222050.png" alt="image-20230901183222050"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901183240178.png" alt="image-20230901183240178"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901183301618.png" alt="image-20230901183301618"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901183315009.png" alt="image-20230901183315009"></p>
<h4 id="梯度下降算法分类"><a href="#梯度下降算法分类" class="headerlink" title="梯度下降算法分类"></a>梯度下降算法分类</h4><ul>
<li><p>全梯度下降算法FGD</p>
<ul>
<li>每次迭代时，使用<strong>全部样本</strong>的梯度值<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/image-20250426183907177.png" alt="image-20250426183907177"></li>
<li>有m个样本，求梯度时用了所有m个样本</li>
<li>特点：慢</li>
</ul>
</li>
<li><p>随机梯度下降算法SGD</p>
<ul>
<li><p>每次迭代时, <strong>随机</strong>选择并使用<strong>一个样本</strong>梯度值<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/image-20250426183754464.png" alt="image-20250426183754464"></p>
</li>
<li><p>特点：简单，高效，不稳定，遇到噪声就ji</p>
</li>
</ul>
</li>
<li><p>小批量梯度下降算法 mini-bantch</p>
<ul>
<li>每次迭代时, <strong>随机选择</strong>并使用小批量的样本梯度值。从m个样本中，选择<strong>x个样本</strong>进行迭代(1&lt;x&lt;m)<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/image-20250426183607791.png" alt="image-20250426183607791"></li>
<li>若batch_size&#x3D;1，则变成了SGD；batch_size&#x3D;n，则变成了FGD</li>
<li>特点：目前使用<strong>最多</strong>，正是因为它避开了 FG 运算<strong>效率低成本大</strong>和 SG <strong>收敛效果不稳定</strong>的缺点</li>
</ul>
</li>
<li><p>随机梯度下降算法SAG</p>
<ul>
<li><p>每次迭代时, 随机选择一个样本的梯度值和<strong>以往样本的梯度值的均值</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/image-20250426184535645-1745664336406-3.png" alt="image-20250426184535645"></p>
<ul>
<li>随机选择一个样本，假设选择 D 样本，计算其梯度值并存储到列表：[D]， 然后使用列表中的梯度值均值，更新模型参数。</li>
<li>随机再选择一个样本，假设选择 G 样本，计算其梯度值并存储到列表： [D, G]，然后使用列表中的梯度值均值，更新模型参数。</li>
<li>随机再选择一个样本，假设又选择了 D 样本, 重新计算该样本梯度值，并 更新列表中 D 样本的梯度值，使用列表中梯度值均值，更新模型参数。</li>
<li>…以此类推，直到算法收敛。</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li>训练初期<strong>表现不佳</strong>，优化<strong>速度较慢</strong>。这是因为我们<strong>常将初始梯度设为0</strong>， 而 SAG 每轮梯度更新都结合了上一轮梯度值。</li>
</ul>
</li>
</ul>
<h4 id="正规方程和梯度下降算法的对比"><a href="#正规方程和梯度下降算法的对比" class="headerlink" title="正规方程和梯度下降算法的对比"></a>正规方程和梯度下降算法的对比</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230901162716376.png" alt="image-20230901162716376"></p>
<h1 id="回归评估方法"><a href="#回归评估方法" class="headerlink" title="回归评估方法"></a>回归评估方法</h1><p><strong>为什么要进行线性回归模型的评估</strong></p>
<p>我们希望衡量预测值和真实值之间的差距，</p>
<p>会用到<strong>MAE、MSE、RMSE多种测评函数</strong>进行评价</p>
<h3 id="平均绝对误差"><a href="#平均绝对误差" class="headerlink" title="平均绝对误差"></a>平均绝对误差</h3><p><strong>Mean Absolute Error (MAE)</strong></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="images/mae.png" alt="img" style="zoom:33%;" />

<ul>
<li>MAE <strong>越小</strong>模型<strong>预测越准确</strong></li>
</ul>
<p>Sklearn 中MAE的API</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line">mean_absolute_error(y_test,y_predict)</span><br></pre></td></tr></table></figure>

<h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><p>   <strong>Mean Squared Error (MSE)</strong></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="images/mse.png" alt="img" style="zoom:33%;" />

<ul>
<li>上面的公式中：n 为样本数量, y 为实际值, y拔为预测值</li>
<li>RMSE <strong>越小</strong>模型<strong>预测越准确</strong></li>
</ul>
<p>Sklearn 中MSE的API</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line">mean_squared_error(y_test,y_predict)</span><br></pre></td></tr></table></figure>

<h3 id="均方根误差"><a href="#均方根误差" class="headerlink" title="均方根误差"></a>均方根误差</h3><p><strong>Root Mean Squared Error (RMSE)</strong></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="images/rmse.png" alt="img" style="zoom: 33%;" />

<ul>
<li>上面的公式中：n 为样本数量, y 为实际值, y拔为预测值</li>
<li>RMSE <strong>越小</strong>模型<strong>预测越准确</strong></li>
</ul>
<h3 id="三种指标的比较"><a href="#三种指标的比较" class="headerlink" title="三种指标的比较"></a>三种指标的比较</h3><p>我们绘制了一条直线 <strong>y &#x3D; 2x +5</strong> 用来拟合 <strong>y &#x3D; 2x + 5 + e.</strong> 这些数据点，其中e为噪声</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="images/rmse2.png" alt="img" style="zoom:80%;" />

<p>从上图中我们发现 <strong>MAE 和 RMSE 非常接近</strong>，都表明模型的误差很低**（MAE 或 RMSE 越小，误差越小！）**。 但是MAE 和 RMSE 有什么区别？为什么MAE较低？</p>
<ul>
<li>对比MAE 和 RMSE的公式，RMSE的计算公式中有一个平方项，<strong>因此：大的误差将被平方，因此会增加 RMSE 的值</strong></li>
<li>可以得出结论，<strong>RMSE 会放大</strong>预测<strong>误差</strong>较大的样本对结果的影响，而 <strong>MAE</strong> 只是给出了<strong>平均误差</strong></li>
<li>由于 RMSE 对误差的 <strong>平方和求平均</strong> 再开根号，大多数情况下<strong>RMSE&gt;MAE</strong></li>
</ul>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul>
<li>都能反映出预测值和真实值之间的误差</li>
<li><strong>MAE（平均绝对误差）<strong>对误差大小</strong>不敏感</strong></li>
<li><strong>RMSE（均方根误差）<strong>会</strong>放大</strong>预测误差较大的样本的影响</li>
</ul>
<h2 id="波士顿房价预测案例"><a href="#波士顿房价预测案例" class="headerlink" title="波士顿房价预测案例"></a>波士顿房价预测案例</h2><h3 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sklearn.linear_model.LinearRegression(fit_intercept=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>通过正规方程优化</li>
<li>参数：<strong><code>fit_intercept</code></strong>，是否计算偏置</li>
<li>属性：<strong><code>LinearRegression.coef_</code> <strong>（回归系数）</strong><code>LinearRegression.intercept_</code></strong>（偏置）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sklearn.linear_model.SGDRegressor(loss=<span class="string">&quot;squared_loss&quot;</span>, fit_intercept=<span class="literal">True</span>, learning_rate =<span class="string">&#x27;constant&#x27;</span>, eta0=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>loss=&quot;squared_loss&quot;</code></strong><ul>
<li><strong>损失函数</strong>：均方误差（MSE），用于回归任务，计算预测值与真实值差的平方和，适合拟合连续型数据。</li>
</ul>
</li>
<li><strong><code>fit_intercept=True</code></strong><ul>
<li>是否计算截距<ul>
<li><code>True</code>：模型包含截距项（即偏置项 b）。</li>
<li><code>False</code>：模型无截距，强制拟合过原点的直线（(y &#x3D; wX)）。</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>learning_rate=&#39;constant&#39;</code></strong><ul>
<li>学习率策略<ul>
<li><code>&#39;constant&#39;</code>：固定学习率，由 <code>eta0</code> 指定。</li>
<li>其他可选值：<ul>
<li><code>&#39;optimal&#39;</code>：随迭代自适应调整（需设置 <code>alpha</code>，适用于凸损失函数）。</li>
<li><code>&#39;invscaling&#39;</code>：按 <code>eta0 / sqrt(t)</code> 衰减（<code>t</code> 为迭代次数）。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><strong><code>eta0=0.01</code></strong><ul>
<li><strong>初始学习率</strong>：固定学习率的值为 <code>0.01</code>。</li>
</ul>
</li>
</ul>
<h3 id="波士顿房价预测"><a href="#波士顿房价预测" class="headerlink" title="波士顿房价预测"></a>波士顿房价预测</h3><h4 id="案例背景介绍"><a href="#案例背景介绍" class="headerlink" title="案例背景介绍"></a>案例背景介绍</h4><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="images/006tNbRwly1ga8u37zooxj317g0tc7dk.jpg" style="zoom:50%;" />

<h4 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h4><p>回归当中的<strong>数据大小不一致</strong>，是否会导致结果影响较大。所以需要做<strong>标准化处理</strong>。</p>
<ul>
<li>数据分割与标准化处理</li>
<li>回归预测</li>
<li>线性回归的算法效果评估</li>
</ul>
<h4 id="回归性能评估"><a href="#回归性能评估" class="headerlink" title="回归性能评估"></a>回归性能评估</h4><p>均方误差(Mean Squared Error, MSE)评价机制：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="images/image-20240731190903826.png" alt="image-20240731190903826" style="zoom: 67%;" />

<p>sklearn中的API：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_error(y_true, y_pred)</span><br></pre></td></tr></table></figure>

<ul>
<li>均方误差回归损失</li>
<li>y_true:真实值</li>
<li>y_pred:预测值</li>
<li>return:浮点数结果</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p><strong>正规方程法</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 导入依赖包</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据预处理</span></span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">data_url = <span class="string">&quot;http://lib.stat.cmu.edu/datasets/boston&quot;</span></span><br><span class="line">raw_df = pd.read_csv(data_url, sep=<span class="string">&quot;\s+&quot;</span>, skiprows=<span class="number">22</span>,  header=<span class="literal">None</span>)</span><br><span class="line">data = np.hstack([raw_df.values[::<span class="number">2</span>, :], raw_df.values[<span class="number">1</span>::<span class="number">2</span>, :<span class="number">2</span>]])</span><br><span class="line">target = raw_df.values[<span class="number">1</span>::<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集划分</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data,  target, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化</span></span><br><span class="line">transfer = StandardScaler()</span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 模型训练，机器学习 - 线性回归</span></span><br><span class="line"><span class="comment"># 3.1 实例化模型(正规方程)</span></span><br><span class="line">estimator = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.2 模型训练</span></span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 模型预测</span></span><br><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测值为:&quot;</span>, y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型的权重系数为:&quot;</span>, estimator.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 模型评估，均方误差</span></span><br><span class="line">error = mean_squared_error(y_test, y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;误差为:&quot;</span>, error)</span><br><span class="line">   </span><br></pre></td></tr></table></figure>

<ul>
<li><code>data_url = &quot;http://lib.stat.cmu.edu/datasets/boston&quot;</code> 定义了数据的 URL 地址，数据是波士顿房价相关数据。</li>
<li><code>raw_df = pd.read_csv(data_url, sep=&quot;\s+&quot;, skiprows = 22, header=None)</code> 使用 <code>pandas</code> 的 <code>read_csv</code> 函数读取数据。<code>sep=&quot;\s+&quot;</code> 表示数据列之间的<strong>分隔符****是一个或多个空白字符</strong>；<code>skiprows = 22</code> 表示<strong>跳过前 22 行数据</strong>；<code>header=None</code> 表示数据文件没有表头。</li>
<li><code>data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, 2]])</code> 使用 <code>numpy</code> 的 <code>hstack</code> 函数将<strong>数据进行拼接</strong>。<code>raw_df.values[::2, :]</code> 取原始数据每隔一行的所有列，<code>raw_df.values[1::2, 2]</code> 取原始数据从第二行开始每隔一行的第三列（索引为 2），然后水平拼接。</li>
<li><code>target = raw_df.values[1::2, 2]</code> 单独提取出目标值，即从第二行开始每隔一行的第三列数据，作为要预测的目标。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.数据预处理</span></span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">data_url = <span class="string">&quot;http://lib.stat.cmu.edu/datasets/boston&quot;</span></span><br><span class="line">raw_df = pd.read_csv(data_url, sep=<span class="string">&quot;\s+&quot;</span>, skiprows=<span class="number">22</span>,  header=<span class="literal">None</span>)</span><br><span class="line">data = np.hstack([raw_df.values[::<span class="number">2</span>, :], raw_df.values[<span class="number">1</span>::<span class="number">2</span>, :<span class="number">2</span>]])</span><br><span class="line">target = raw_df.values[<span class="number">1</span>::<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集划分</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data,  target, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化</span></span><br><span class="line">transfer = StandardScaler()</span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.模型训练</span></span><br><span class="line"><span class="comment"># 实例化模型--梯度下降法</span></span><br><span class="line">estimator = SGDRegressor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.模型预测</span></span><br><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测值为:&quot;</span>, y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型的权重系数为:&quot;</span>, estimator.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型的偏置为:&quot;</span>, estimator.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.模型评估, 均方误差</span></span><br><span class="line">error = mean_squared_error(y_test, y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;误差为:&quot;</span>, error)</span><br></pre></td></tr></table></figure>



<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h3><p>过拟合和欠拟合的区别：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="images/006tNbRwly1ga8u2rlw69j315m0oc40y.jpg" alt="æ¬ æåè¿æåå¾ç¤º" style="zoom: 33%;" />

<ul>
<li><p>欠拟合在<strong>训练集和测试集</strong>上的<strong>误差都较大</strong></p>
</li>
<li><p>过拟合在<strong>训练集上误差较小</strong>，而<strong>测试集上误差较大</strong></p>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/image-20230913101352444.png" alt="image-20230913101352444"></p>
<p><strong>欠拟合</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">666</span>)</span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line">y = <span class="number">0.5</span> * x ** <span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化线性回归模型</span></span><br><span class="line">estimator = LinearRegression()</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>) <span class="comment">#将数组 x 转换为列向量（二维数组）（-1换n）</span></span><br><span class="line">estimator.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">y_predict = estimator.predict(X)</span><br></pre></td></tr></table></figure>



<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/1.png" alt="1"></p>
<p><strong>刚好拟合</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimator = LinearRegression()</span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为数据添加二次项特征，将原始特征 X 和其平方项按列拼接</span></span><br><span class="line">X2 = np.hstack([X, X ** <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用添加二次项特征后的 X2 和目标值 y 训练线性回归模型</span></span><br><span class="line">estimator.fit(X2, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">y_predict = estimator.predict(X2)</span><br></pre></td></tr></table></figure>



<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/2.png" alt="2"></p>
<p><strong>过拟合</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimator = LinearRegression()</span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为数据添加高次项</span></span><br><span class="line">X2 = np.hstack([X, X ** <span class="number">2</span>, X ** <span class="number">3</span>, X ** <span class="number">4</span>, X ** <span class="number">5</span>, X ** <span class="number">6</span>, X ** <span class="number">7</span>, X ** <span class="number">8</span>, X ** <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用添加高次项特征后的 X2 和目标值 y 训练线性回归模型</span></span><br><span class="line">estimator.fit(X2, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">y_predict = estimator.predict(X2)</span><br></pre></td></tr></table></figure>



<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/3.png"></p>
<h3 id="原因以及解决办法"><a href="#原因以及解决办法" class="headerlink" title="原因以及解决办法"></a>原因以及解决办法</h3><ul>
<li><p><strong>欠拟合产生原因：</strong> 学习到数据的<strong>特征过少</strong></p>
<ul>
<li>解决办法：<ul>
<li><strong>1）添加其他特征项</strong>：有时出现欠拟合是因为特征项不够导致的，可以添加其他特征项来解决</li>
<li><strong>添加多项式特征</strong>：模型过于简单时的常用套路，例如将线性模型通过<strong>添加二次项</strong>或<strong>三次项</strong>使模型泛化能力更强</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>过拟合产生原因：</strong> 原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾所有测试样本</p>
<ul>
<li><p>解决办法：</p>
<ul>
<li><strong>重新清洗数据</strong>：导致过拟合的一个原因有可能是数据不纯，如果出现了过拟合就需要重新清洗数据。</li>
</ul>
<ul>
<li><p><strong>增大数据的训练量</strong>：还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</p>
</li>
<li><p><strong>正则化</strong></p>
</li>
<li><p><strong>减少特征维度</strong></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="正则化-1"><a href="#正则化-1" class="headerlink" title="正则化"></a>正则化</h3><p>在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量<strong>减少这个特征的影响（甚至删除某个特征的影响）</strong>，这就是<strong>正则化</strong></p>
<h4 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a><strong>L1正则化</strong></h4><ul>
<li><p>假设𝐿(𝑊)是未加正则项的损失，𝜆是一个超参，控制正则化项的大小。</p>
</li>
<li><p>则最终的损失函数：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="images/image-20240731190649914.png" alt="image-20240731190649914" style="zoom:50%;" />

<ul>
<li>α 叫做<strong>惩罚系数</strong>，该值越大则权重调整的幅度越大，即：表示对特征权重惩罚力度就越大</li>
<li>L1 正则化会使得权重趋向于 0，<strong>甚至等于 0</strong>，使得<strong>某些特征失效</strong>，达到<strong>特征筛选</strong>的目的</li>
</ul>
</li>
</ul>
<p><strong>L1正则为什么可以产生稀疏解</strong></p>
<ul>
<li>L1正则化的梯度包含<strong>符号函数sign</strong>无论参数大小如何，<strong>惩罚力度恒定</strong>。当参数的原始<strong>梯度绝对值小于λ</strong>时，参数会被直接压缩至零</li>
</ul>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="images/l2_4.png" alt="l2" style="zoom:50%;" />

<p><strong>LASSO回归:</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br></pre></td></tr></table></figure>

<h4 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a><strong>L2正则化</strong></h4><ul>
<li><p>假设𝐿(𝑊)是未加正则项的损失，𝜆是一个超参，控制正则化项的大小。</p>
</li>
<li><p>则最终的损失函数：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./assets/image-20250426195500446.png" alt="image-20250426195500446"></p>
<ul>
<li>α 叫做惩罚系数，该值越大则权重调整的幅度就越大，即：表示对特征权重惩罚力度就越大 </li>
<li>L2 正则化会使得<strong>权重趋向于 0</strong>，<strong>一般不等于 0</strong></li>
</ul>
</li>
</ul>
<p>Ridge回归: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br></pre></td></tr></table></figure>

<h4 id="正则化案例"><a href="#正则化案例" class="headerlink" title="正则化案例"></a><strong>正则化案例</strong></h4><p>对过拟合模型L1正则化调整，会将高次方系数变为0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据 x, y(增加上噪声)</span></span><br><span class="line">np.random.seed(<span class="number">666</span>)</span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line">y = <span class="number">0.5</span> * x ** <span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="comment"># 实例化 L1 正则化模型 做实验:alpha 惩罚力度越来越大,k 值越来越小,返回会欠拟合</span></span><br><span class="line">estimator = Lasso(alpha=<span class="number">0.005</span>, normalize=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">X3 = np.hstack([X, X ** <span class="number">2</span>, X ** <span class="number">3</span>, X ** <span class="number">4</span>, X ** <span class="number">5</span>, X ** <span class="number">6</span>, X ** <span class="number">7</span>, X ** <span class="number">8</span>, X ** <span class="number">9</span>, X ** <span class="number">10</span>])  <span class="comment"># 数据增加二次项</span></span><br><span class="line">estimator.fit(X3, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;estimator.coef_&#x27;</span>, estimator.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">y_predict = estimator.predict(X3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评估，计算均方误差</span></span><br><span class="line"><span class="comment"># 模型评估 MSE</span></span><br><span class="line">myret = mean_squared_error(y, y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;myret--&gt;&#x27;</span>, myret)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/l2_6.png" alt="img"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据 x, y(增加噪声)</span></span><br><span class="line">np.random.seed(<span class="number">666</span>)</span><br><span class="line">x = np.random.uniform(-<span class="number">3</span>, <span class="number">3</span>, size=<span class="number">100</span>)</span><br><span class="line">y = <span class="number">0.5</span> * x ** <span class="number">2</span> + x + <span class="number">2</span> + np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="comment"># 实例化 L2 正则化模型</span></span><br><span class="line">estimator = Ridge(alpha=<span class="number">0.005</span>, normalize=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line">X = x.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">X3 = np.hstack([X, X ** <span class="number">2</span>, X ** <span class="number">3</span>, X ** <span class="number">4</span>, X ** <span class="number">5</span>, X ** <span class="number">6</span>, X ** <span class="number">7</span>, X ** <span class="number">8</span>, X ** <span class="number">9</span>, X ** <span class="number">10</span>])</span><br><span class="line">estimator.fit(X3, y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;estimator.coef_&#x27;</span>, estimator.coef_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line">y_predict = estimator.predict(X3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型评估，计算均方误差</span></span><br><span class="line">myret = mean_squared_error(y, y_predict)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;myret--&gt;&#x27;</span>, myret)</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y)</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/images/l2_7.png" alt="img"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://Sre-Night.github.io">Sre.Night</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://sre-night.github.io/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">https://sre-night.github.io/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://Sre-Night.github.io" target="_blank">Sre.Night's blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/05/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" title="逻辑回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">逻辑回归</div></div><div class="info-2"><div class="info-item-1">逻辑回归逻辑回归简介 逻辑回归是解决二分类问题的利器 数学知识sigmoid函数   概率 极大似然估计核心思想： 设模型中含有待估参数w，可以取很多值。已经知道了样本观测值，从w的一切可能值中（选出一个使该观察值出现的概率为最大的值，作为w参数的估计值，这就是极大似然估计。（顾名思义：就是看上去那个是最大可能的意思） 举个例子： 假设有一枚不均匀的硬币，出现正面的概率和反面的概率是不同的。假定出现正面的概率为𝜃， 抛了6次得到如下现象 D &#x3D; {正面，反面，反面，正面，正面，正面}。每次投掷事件都是相互独立的。 则根据产生的现象D，来估计参数𝜃是多少? P(D|𝜃) = P &#123;正面，反面，反面，正面，正面，正面&#125; = P(正面|𝜃) P(正面|𝜃) P(正面|𝜃) P(正面|𝜃) P(正面|𝜃) P(正面|𝜃)=𝜃 *(1-𝜃)*(1-𝜃)𝜃*𝜃*𝜃 = 𝜃4(1 − 𝜃)  问题转化为:求此函数的极大值时，估计𝜃为多少  对数函数 逻辑回归原理学习目标1.理解逻辑回归算法的原理 2.知道逻辑回归的损失函数 原理...</div></div></div></a><a class="pagination-related" href="/2025/04/27/linux%E5%9F%BA%E7%A1%80%E5%8F%8Adocker/" title="LINUX基础及DOCKER"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">LINUX基础及DOCKER</div></div><div class="info-2"><div class="info-item-1">LINUX基础及DOCKER一、Linux基础Linux是一个免费的、开源的类 UNIX 操作系统，在 1991 由林纳斯·托瓦兹在赫尔辛基大学上学时创立。Linux有上百种不同的发行版，如基于社区开发的debian、archlinux，和基于商业开发的Red Hat Enterprise Linux、SUSE、Oracle Linux等。目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debian、Fedora、SuSE、OpenSUSE、Arch Linux、SolusOS等。 1. 系统目录结构在 Linux 或 Unix 操作系统中，所有的文件和目录都被组织成以一个根节点开始的倒置的树状结构。文件系统的最顶层是由根目录开始的，系统使用/来表示根目录。在根目录之下的既可以是目录，也可以是文件，而每一个目录中又可以包含子目录文件。进入系统后在桌面点击鼠标右键，选择Open Terminal打开一个命令行窗口，这个命令行窗口也叫“终端”。在终端中输入ls /可以看到如下的文件及目录。  /bin：存放着最经常使用的命令。 /boot：存放启动 Linux ...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Sre.Night</div><div class="author-info-description">谁怕？一蓑烟雨任平生。</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Sre-Night"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Sre-Night" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2895487980@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的个人博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.</span> <span class="toc-text">线性回归介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1"><span class="toc-number">2.0.1.</span> <span class="toc-text">线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB"><span class="toc-number">2.0.2.</span> <span class="toc-text">线性回归分类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E5%BD%92%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3"><span class="toc-number">3.</span> <span class="toc-text">线归问题求解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E5%BD%92API%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">3.0.1.</span> <span class="toc-text">线归API的应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">3.0.2.</span> <span class="toc-text">损失函数(代价函数)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0%E5%92%8C%E7%9F%A9%E9%98%B5"><span class="toc-number">3.0.3.</span> <span class="toc-text">导数和矩阵</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%95%B0%E6%8D%AE%E8%A1%A8%E8%BF%B0"><span class="toc-number">3.0.3.1.</span> <span class="toc-text">常见的数据表述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0"><span class="toc-number">3.0.3.2.</span> <span class="toc-text">导数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%81%8F%E5%AF%BC"><span class="toc-number">3.0.3.3.</span> <span class="toc-text">偏导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%91%E9%87%8F"><span class="toc-number">3.0.3.4.</span> <span class="toc-text">向量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5"><span class="toc-number">3.0.3.5.</span> <span class="toc-text">矩阵</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="toc-number">3.0.4.</span> <span class="toc-text">一元线性回归的解析解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%B3%95"><span class="toc-number">3.0.5.</span> <span class="toc-text">多元线性回归-正规方程法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">3.0.6.</span> <span class="toc-text">梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%80%9D%E6%83%B3"><span class="toc-number">3.0.6.1.</span> <span class="toc-text">梯度下降思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%93%B6%E8%A1%8C%E4%BF%A1%E8%B4%B7%E6%A1%88%E4%BE%8B"><span class="toc-number">3.0.6.2.</span> <span class="toc-text">银行信贷案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB"><span class="toc-number">3.0.6.3.</span> <span class="toc-text">梯度下降算法分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-number">3.0.6.4.</span> <span class="toc-text">正规方程和梯度下降算法的对比</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">回归评估方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">4.0.1.</span> <span class="toc-text">平均绝对误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">4.0.2.</span> <span class="toc-text">均方误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E6%A0%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">4.0.3.</span> <span class="toc-text">均方根误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E7%A7%8D%E6%8C%87%E6%A0%87%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">4.0.4.</span> <span class="toc-text">三种指标的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">4.0.5.</span> <span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%E6%A1%88%E4%BE%8B"><span class="toc-number">4.1.</span> <span class="toc-text">波士顿房价预测案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92API"><span class="toc-number">4.1.1.</span> <span class="toc-text">线性回归API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B"><span class="toc-number">4.1.2.</span> <span class="toc-text">波士顿房价预测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D"><span class="toc-number">4.1.2.1.</span> <span class="toc-text">案例背景介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90"><span class="toc-number">4.1.2.2.</span> <span class="toc-text">案例分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0"><span class="toc-number">4.1.2.3.</span> <span class="toc-text">回归性能评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.1.2.4.</span> <span class="toc-text">代码实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">4.2.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">4.2.1.</span> <span class="toc-text">欠拟合与过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%9B%A0%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="toc-number">4.2.2.</span> <span class="toc-text">原因以及解决办法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96-1"><span class="toc-number">4.2.3.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">4.2.3.1.</span> <span class="toc-text">L1正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#L2%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">4.2.3.2.</span> <span class="toc-text">L2正则化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E6%A1%88%E4%BE%8B"><span class="toc-number">4.2.3.3.</span> <span class="toc-text">正则化案例</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/18/VPN%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8/" title="VPN技术原理与应用">VPN技术原理与应用</a><time datetime="2025-09-18T04:54:00.000Z" title="发表于 2025-09-18 12:54:00">2025-09-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/23/%E5%8F%98%E9%87%8F%E4%BB%8B%E7%BB%8D/" title="初识python">初识python</a><time datetime="2025-06-23T06:52:13.000Z" title="发表于 2025-06-23 14:52:13">2025-06-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/23/%E5%88%A4%E6%96%AD%E5%BE%AA%E7%8E%AF/" title="循环与判断">循环与判断</a><time datetime="2025-06-23T06:52:13.000Z" title="发表于 2025-06-23 14:52:13">2025-06-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" title="逻辑回归">逻辑回归</a><time datetime="2025-05-09T06:22:11.000Z" title="发表于 2025-05-09 14:22:11">2025-05-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/01/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="线性回归">线性回归</a><time datetime="2025-05-01T11:25:32.000Z" title="发表于 2025-05-01 19:25:32">2025-05-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer-left"><div class="footer-title"><span>Sre.Night's blog | </span><span class="footer-copyright">&copy;2025 By Sre.Night</span></div><div class="wordcount"></div><span>皆安 已经写了 41.7k 字，</span><span>好像写完一本 海明威 的 《老人与海》 了啊</span></div><div id="footer-right"><div class="footer-totop"><i class="fas fa-chevron-up" onclick="rmf.scrollToTop()"></i></div><div class="footer-info"><p>使用Hexo框架 | 基于butterfly修改</p></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><script defer src="/js/cursor.js"></script><script defer src="/js/nav.js"></script><script defer src="/js/title.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 380px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 320px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(/img/category-card-bg/honghong.jpg);"> <a class="categoryBar-list-link" href="categories/web安全/">web安全</a><span class="categoryBar-list-count">7</span><span class="categoryBar-list-descr">记录学习历程</span></li><li class="categoryBar-list-item" style="background:url(/img/category-card-bg/wangquan.jpeg);"> <a class="categoryBar-list-link" href="categories/大模型/">大模型</a><span class="categoryBar-list-count">4</span><span class="categoryBar-list-descr">记录学习历程</span></li><li class="categoryBar-list-item" style="background:url(/img/category-card-bg/rongrong.jpg);"> <a class="categoryBar-list-link" href="categories/python/">python</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr">记录学习历程</span></li></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>